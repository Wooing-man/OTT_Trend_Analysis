{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMcN7jfFEIO6GHIqPzqjlO9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### OTT별 토픽모델링\n","\n","- 토큰화 함수화시키기\n","- LDA 적용 및 시각화까지 추출\n","- 2020 ~ 2023에 대한 토픽 변화 확인\n","\n","\\\n","\n","+향후 진행\n","1. 사전학습모델 완료되면 OTT별 리뷰에 감성분석 라벨링 달기 (수작업으로 어느 정도 라벨링 붙인 후 정확도 비교해야할 듯)\n","2. 긍부정 나눈 뒤 토팍 모델링 진행\n","3. 토픽별로 임베딩\n","4. 시간에 따라 임베딩"],"metadata":{"id":"6zzH1UYhqnbO"}},{"cell_type":"markdown","source":["## 설치"],"metadata":{"id":"f-FlzAYgq7JM"}},{"cell_type":"code","source":["# Scraper 설치\n","# !pip install app_store_scraper\n","# !pip install google-play-scraper\n","!pip install pyLDAvis # LDA 시각화\n","!pip install git+https://github.com/haven-jeon/PyKoSpacing.git # 한국어 띄어쓰기 패키지(pykospacing)\n","!pip install git+https://github.com/ssut/py-hanspell.git # 한국어 Norm : 맞춤법 교정(hanspell)\n","\n","# Colab에 Mecab 설치\n","!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n","%cd Mecab-ko-for-Google-Colab\n","!bash install_mecab-ko_on_colab190912.sh\n","!pip install konlpy\n","\n","! pip install MeCab\n","# 오류나면 [런타임 다시 시작] 누르고 다시 설치 필요 없이 진행 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"],"metadata":{"id":"YXDo8ArxqnN0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 라이브러리"],"metadata":{"id":"IP0EU44WrKeX"}},{"cell_type":"code","source":["import re\n","import time\n","import numpy as np\n","import pandas as pd\n","import datetime\n","import warnings # 경고 메시지 무시\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm \n","warnings.filterwarnings(action='ignore')\n","\n","# Tokenize\n","# from pykospacing import Spacing # 띄어쓰기\n","from gensim import corpora # 단어 빈도수 계산 패키지\n","from sklearn.decomposition import LatentDirichletAllocation\n","import gensim # LDA 모델 활용 목적\n","import pyLDAvis.gensim_models # LDA 시각화용 패키지\n","from collections import Counter # 단어 등장 횟수 카운트\n","\n","# LDA\n","from gensim.models.ldamodel import LdaModel\n","from gensim.models.callbacks import CoherenceMetric\n","from gensim import corpora\n","from gensim.models.callbacks import PerplexityMetric\n","from gensim.models import CoherenceModel\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","# 한국어 형태소 분석기 중 성능이 가장 우수한 Mecab 사용\n","from konlpy.tag import *\n","mecab = Mecab()\n","\n","# Data 불러오기\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3t40qWTqnLg","executionInfo":{"status":"ok","timestamp":1683524617178,"user_tz":-540,"elapsed":23380,"user":{"displayName":"David Kim","userId":"08045787072475879019"}},"outputId":"c03ebc5f-40d6-4a9d-ce0f-2994e13f573a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pexpect/popen_spawn.py:60: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n","  self._read_thread.setDaemon(True)\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dl-BLKdXN5OT","executionInfo":{"status":"ok","timestamp":1683524626184,"user_tz":-540,"elapsed":303,"user":{"displayName":"David Kim","userId":"08045787072475879019"}},"outputId":"f8b12694-56b0-4243-f8b1-19852359e626"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","source":["## 함수"],"metadata":{"id":"liQnOjVLrfdf"}},{"cell_type":"code","source":["## ------------------ 데이터 전처리 ------------------ ##\n","def date_extract(dataset, ago_year=3, printing=False):\n","  \"\"\"\n","  Input: OTT별 전체 데이터프레임\n","  Output: 최근 n년의 기간에 해당하는 데이터 추출, 결측처리 완료\n","  \"\"\"\n","\n","  dataset['at'] = pd.to_datetime(dataset['at'])\n","  dataset['year'] = dataset['at'].dt.year\n","  dataset = dataset[dataset['year'] >= (dataset['year'].unique().max()-ago_year)]\n","  dataset = dataset.dropna(subset=['content'])\n","  \n","  if printing:\n","    for y in dataset['year'].unique():\n","      tmp = dataset[dataset['year'] == y]\n","      print(f'{y}: {len(tmp)} rows\\n')\n","  return dataset\n","\n","\n","class Data_processing:\n","  '''\n","  LDA 적용을 위한 모든 전처리\n","  '''\n","  def __init__(self, dataset):\n","    self.dataset = dataset\n","    self.replace_list = pd.read_excel('./replace_list.xlsx')\n","    self.stopwords_list = list(pd.read_excel('./stopword_list.xlsx')['stopword'])\n","\n","  def ko_language(self, text):\n","    # 한글 외 문자 제거 & 띄어쓰기 맞추기\n","    hangul = re.compile('[^가-힣 ]')\n","    result = hangul.sub('', text)\n","    return result\n","\n","  def replace_word(self, text):\n","    # 단어 치환\n","    for i, word in enumerate(self.replace_list['before_replacement']):\n","      if word in text:\n","        result = re.sub(word, self.replace_list['after_replacement'][i], text)\n","        return result\n","    return text\n","\n","  def tokenize(self, text):\n","    # 토큰화\n","    return mecab.nouns(text)\n","\n","  def remove_stopwords(self, text, add_stopwords=None):\n","    # 불용어제거\n","    if add_stopwords:\n","      self.stopwords_list += add_stopwords\n","    result = [x for x in text if x not in self.stopwords_list]\n","    return result\n","  \n","  def select_review(self, data, min_token_n:int, max_token_n:int):\n","    # 특정 토큰 개수의 리뷰 선별\n","    remove_idx_list = []\n","    for i in range(len(data)):\n","      if min_token_n <= len(data.iloc[i]['review_prep']) <= max_token_n:\n","        continue\n","      else:\n","        remove_idx_list.append(i)\n","\n","    return data.drop(remove_idx_list, axis=0)\n","\n","  def get_token(self, add_stopwords=None, min_token_n:int=3, max_token_n:int=1000):\n","    self.dataset['review_prep'] = self.dataset['content'].apply(lambda x:self.ko_language(x))\n","    self.dataset = self.dataset.reset_index(drop=True)\n","    print('ko_language done..')\n","    self.dataset['review_prep'] = self.dataset['review_prep'].apply(lambda x:self.replace_word(x))\n","    self.dataset = self.dataset.reset_index(drop=True)\n","    print('replace_word done..')\n","    self.dataset['review_prep'] = self.dataset['review_prep'].apply(lambda x:self.tokenize(x))\n","    self.dataset = self.dataset.reset_index(drop=True)\n","    print('tokenize done..')\n","    self.dataset['review_prep'] = self.dataset['review_prep'].apply(lambda x:self.remove_stopwords(x, add_stopwords))\n","    self.dataset = self.dataset.reset_index(drop=True)\n","    print('remove_stopwords done..')\n","    self.dataset = self.select_review(self.dataset, min_token_n, max_token_n)\n","    return list(self.dataset['review_prep']), self.dataset\n","\n","\n","\n","## ------------------ 모델링 ------------------ ##\n","class Model:\n","  '''\n","  LDA 모델\n","\n","  no_below = 분석에 사용할 단어의 최소 빈도 수 제약 (ex) 2이면, 빈도가 최소 2이상 넘어간 단어만 취급)\n","  no_above = 전체의 몇 %로 이상 차지하는 단어를 필터링 할 것인지?\n","  '''\n","  def __init__(self, inputs, num_topics:int, no_below:int=2):\n","    self.dictionary = corpora.Dictionary(inputs)\n","    self.dictionary.filter_extremes(no_below=no_below)\n","    self.corpus = [self.dictionary.doc2bow(x) for x in inputs]\n","    self.inputs = inputs\n","    self.num_topics = num_topics\n","  \n","  def LDA_model(self, chunksize=2000, passes=20, iterations=400, eval_every=None):\n","    '''\n","    num_topics: 생성될 토픽의 개수\n","    chunksize: 한번의 트레이닝에 처리될 문서의 개수\n","    passes: 딥러닝에서 Epoch와 같은 개념으로, 전체 corpus로 모델 학습 횟수 결정\n","    interations: 문서 당 반복 횟수\n","    '''\n","    temp = self.dictionary[0]\n","    id2word = self.dictionary.id2token\n","\n","    self.model = LdaModel(\n","      corpus=self.corpus,\n","      id2word=id2word,\n","      chunksize=chunksize,\n","      alpha='auto',\n","      eta='auto',\n","      iterations=iterations,\n","      num_topics=self.num_topics,\n","      passes=passes,\n","      eval_every=eval_every)\n","\n","\n","  def print_topic_prop(self, topn=10, num_words=20):\n","    self.LDA_model()\n","    topics = self.model.print_topics(num_words=num_words)\n","\n","\n","    # 토픽별 포함 단어 추출\n","    topic_words = {}\n","    for idx, words in topics:\n","      topic_words[idx] = words.split('+')\n","\n","    topic_table = pd.DataFrame(topic_words)\n","    topic_table.columns = [f'topic_{t+1}' for t in range(len(topics))]\n","\n","    # coherence\n","    coherence_model_lda = CoherenceModel(model=self.model, texts=self.inputs, dictionary = self.dictionary, topn=10)\n","    coherence_lda = coherence_model_lda.get_coherence()\n","    print('LDA done..')\n","\n","    return topic_table, coherence_lda, topics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QGqTDVZCqnJs","executionInfo":{"status":"ok","timestamp":1683524628298,"user_tz":-540,"elapsed":305,"user":{"displayName":"David Kim","userId":"08045787072475879019"}},"outputId":"0fa14dbd-304b-4a5e-ec91-81bf085338d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","source":["----"],"metadata":{"id":"NGT4YAZyMfE8"}},{"cell_type":"markdown","source":["## 사용자 정의"],"metadata":{"id":"On6_Xa_sMfnj"}},{"cell_type":"code","source":["# OTT 선택, 전체로 할 시 None 으로 기입\n","# OTT = 'tiving'\n","OTT = None\n","\n","# 토픽 결정 수\n","num_topics = 5\n","\n","# 불용어 \n","add_stopwords_list = ['티빙', '넷플릭스', '웨이브', '쿠팡', '쿠팡플레이', '디즈니', '나무',\n","                      '디즈니플러스', '애플리케이션', '개발자', '도도', '슈슈', '휴휴',\n","                      '옥수수', '라프텔', '라프']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8Hxum21Md9t","executionInfo":{"status":"ok","timestamp":1683524660697,"user_tz":-540,"elapsed":5,"user":{"displayName":"David Kim","userId":"08045787072475879019"}},"outputId":"ed266987-76e2-4b05-b7ee-bd02fdfe2aa5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","source":["## __Main__()"],"metadata":{"id":"o0yMKn5mr-2d"}},{"cell_type":"code","source":["# 데이터 불러오기\n","df = pd.read_csv('google_store_review.csv', encoding='utf-8-sig')\n","if OTT: \n","  df_ott = df[df['ott']== OTT]\n","else:\n","  df_ott = df.copy()\n","\n","\n","### \b리뷰 기간 조정 및 결측치 처리 ###\n","df_ott = date_extract(df_ott, printing=True)\n","\n","### 사전학습모델을 이용한 리뷰 라벨링 ###\n","\n","\n","### 토픽모델링 ###\n","token_data = Data_processing(df_ott) # 전처리 시작\n","tokenized_data, _ = token_data.get_token(add_stopwords_list)\n","lda = Model(tokenized_data, num_topics=num_topics) # 토픽모델링 시작\n","topic_table, coherence, _ = lda.print_topic_prop()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MAcsFlG8qm_P","executionInfo":{"status":"ok","timestamp":1683527419748,"user_tz":-540,"elapsed":1380151,"user":{"displayName":"David Kim","userId":"08045787072475879019"}},"outputId":"1f3bd090-2397-4b30-b109-e7a5ac0cb6d3"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["2023: 3051 rows\n","\n","2022: 16623 rows\n","\n","2021: 22667 rows\n","\n","2020: 17116 rows\n","\n","ko_language done..\n","replace_word done..\n","tokenize done..\n","remove_stopwords done..\n","LDA done..\n"]}]},{"cell_type":"code","source":["topic_table"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":732},"id":"eQIZpK7AY5KT","executionInfo":{"status":"ok","timestamp":1683527820314,"user_tz":-540,"elapsed":565,"user":{"displayName":"David Kim","userId":"08045787072475879019"}},"outputId":"f040bf98-8d7d-41d2-c292-7856efc8cfbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"execute_result","data":{"text/plain":["          topic_1         topic_2         topic_3        topic_4   \n","0      0.050*\"돈\"      0.072*\"화면\"       0.108*\"수\"      0.083*\"거\"   \\\n","1     0.043*\"결제\"      0.068*\"영상\"      0.099*\"영화\"      0.054*\"플\"    \n","2     0.038*\"오류\"      0.056*\"재생\"      0.053*\"확인\"      0.051*\"게\"    \n","3    0.033*\"로그인\"    0.050*\"업데이트\"     0.043*\"컨텐츠\"     0.024*\"오픈\"    \n","4      0.025*\"내\"      0.044*\"불편\"     0.042*\"드라마\"     0.024*\"지원\"    \n","5     0.022*\"회원\"      0.030*\"시청\"      0.028*\"마블\"    0.024*\"한국인\"    \n","6      0.021*\"번\"      0.026*\"개선\"      0.026*\"최신\"     0.023*\"화질\"    \n","7     0.021*\"문제\"       0.019*\"중\"      0.020*\"추가\"      0.022*\"말\"    \n","8     0.021*\"로딩\"      0.018*\"조절\"    0.018*\"프로그램\"     0.022*\"리뷰\"    \n","9     0.020*\"삭제\"      0.015*\"설정\"     0.017*\"콘텐츠\"     0.020*\"연동\"    \n","10    0.017*\"설치\"      0.013*\"배속\"       0.017*\"점\"      0.020*\"전\"    \n","11    0.017*\"가입\"      0.012*\"소리\"      0.014*\"검색\"     0.019*\"유료\"    \n","12    0.016*\"실행\"       0.012*\"시\"      0.014*\"방송\"     0.019*\"티비\"    \n","13     0.016*\"앱\"      0.011*\"회차\"      0.011*\"감사\"      0.019*\"건\"    \n","14     0.015*\"폰\"      0.011*\"가능\"      0.010*\"예능\"      0.018*\"개\"    \n","15     0.014*\"중\"     0.011*\"플레이\"      0.010*\"채널\"     0.017*\"무료\"    \n","16    0.014*\"해결\"       0.011*\"전\"      0.010*\"부족\"      0.016*\"데\"    \n","17    0.013*\"접속\"      0.011*\"버튼\"      0.009*\"작품\"     0.014*\"수정\"    \n","18   0.013*\"플레이\"      0.010*\"팝업\"      0.009*\"만족\"      0.012*\"빙\"    \n","19     0.012*\"짜증\"      0.010*\"밝기\"     0.009*\"시리즈\"      0.012*\"듯\"   \n","\n","           topic_5  \n","0      0.281*\"자막\"   \n","1      0.079*\"한글\"   \n","2      0.056*\"시즌\"   \n","3      0.038*\"심슨\"   \n","4      0.035*\"날자\"   \n","5       0.027*\"개\"   \n","6      0.016*\"영어\"   \n","7      0.014*\"설정\"   \n","8     0.014*\"한국어\"   \n","9       0.011*\"애\"   \n","10   0.011*\"플레이어\"   \n","11     0.010*\"크기\"   \n","12     0.008*\"싱크\"   \n","13      0.008*\"기\"   \n","14     0.006*\"더빙\"   \n","15     0.006*\"수정\"   \n","16     0.006*\"재설\"   \n","17      0.005*\"트\"   \n","18     0.005*\"개판\"   \n","19      0.004*\"배경\"  "],"text/html":["\n","  <div id=\"df-f2405b70-a52a-4c8c-8989-11e26a4eaf29\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>topic_1</th>\n","      <th>topic_2</th>\n","      <th>topic_3</th>\n","      <th>topic_4</th>\n","      <th>topic_5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.050*\"돈\"</td>\n","      <td>0.072*\"화면\"</td>\n","      <td>0.108*\"수\"</td>\n","      <td>0.083*\"거\"</td>\n","      <td>0.281*\"자막\"</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.043*\"결제\"</td>\n","      <td>0.068*\"영상\"</td>\n","      <td>0.099*\"영화\"</td>\n","      <td>0.054*\"플\"</td>\n","      <td>0.079*\"한글\"</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.038*\"오류\"</td>\n","      <td>0.056*\"재생\"</td>\n","      <td>0.053*\"확인\"</td>\n","      <td>0.051*\"게\"</td>\n","      <td>0.056*\"시즌\"</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.033*\"로그인\"</td>\n","      <td>0.050*\"업데이트\"</td>\n","      <td>0.043*\"컨텐츠\"</td>\n","      <td>0.024*\"오픈\"</td>\n","      <td>0.038*\"심슨\"</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.025*\"내\"</td>\n","      <td>0.044*\"불편\"</td>\n","      <td>0.042*\"드라마\"</td>\n","      <td>0.024*\"지원\"</td>\n","      <td>0.035*\"날자\"</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.022*\"회원\"</td>\n","      <td>0.030*\"시청\"</td>\n","      <td>0.028*\"마블\"</td>\n","      <td>0.024*\"한국인\"</td>\n","      <td>0.027*\"개\"</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.021*\"번\"</td>\n","      <td>0.026*\"개선\"</td>\n","      <td>0.026*\"최신\"</td>\n","      <td>0.023*\"화질\"</td>\n","      <td>0.016*\"영어\"</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.021*\"문제\"</td>\n","      <td>0.019*\"중\"</td>\n","      <td>0.020*\"추가\"</td>\n","      <td>0.022*\"말\"</td>\n","      <td>0.014*\"설정\"</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.021*\"로딩\"</td>\n","      <td>0.018*\"조절\"</td>\n","      <td>0.018*\"프로그램\"</td>\n","      <td>0.022*\"리뷰\"</td>\n","      <td>0.014*\"한국어\"</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.020*\"삭제\"</td>\n","      <td>0.015*\"설정\"</td>\n","      <td>0.017*\"콘텐츠\"</td>\n","      <td>0.020*\"연동\"</td>\n","      <td>0.011*\"애\"</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.017*\"설치\"</td>\n","      <td>0.013*\"배속\"</td>\n","      <td>0.017*\"점\"</td>\n","      <td>0.020*\"전\"</td>\n","      <td>0.011*\"플레이어\"</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.017*\"가입\"</td>\n","      <td>0.012*\"소리\"</td>\n","      <td>0.014*\"검색\"</td>\n","      <td>0.019*\"유료\"</td>\n","      <td>0.010*\"크기\"</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0.016*\"실행\"</td>\n","      <td>0.012*\"시\"</td>\n","      <td>0.014*\"방송\"</td>\n","      <td>0.019*\"티비\"</td>\n","      <td>0.008*\"싱크\"</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0.016*\"앱\"</td>\n","      <td>0.011*\"회차\"</td>\n","      <td>0.011*\"감사\"</td>\n","      <td>0.019*\"건\"</td>\n","      <td>0.008*\"기\"</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0.015*\"폰\"</td>\n","      <td>0.011*\"가능\"</td>\n","      <td>0.010*\"예능\"</td>\n","      <td>0.018*\"개\"</td>\n","      <td>0.006*\"더빙\"</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0.014*\"중\"</td>\n","      <td>0.011*\"플레이\"</td>\n","      <td>0.010*\"채널\"</td>\n","      <td>0.017*\"무료\"</td>\n","      <td>0.006*\"수정\"</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0.014*\"해결\"</td>\n","      <td>0.011*\"전\"</td>\n","      <td>0.010*\"부족\"</td>\n","      <td>0.016*\"데\"</td>\n","      <td>0.006*\"재설\"</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0.013*\"접속\"</td>\n","      <td>0.011*\"버튼\"</td>\n","      <td>0.009*\"작품\"</td>\n","      <td>0.014*\"수정\"</td>\n","      <td>0.005*\"트\"</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0.013*\"플레이\"</td>\n","      <td>0.010*\"팝업\"</td>\n","      <td>0.009*\"만족\"</td>\n","      <td>0.012*\"빙\"</td>\n","      <td>0.005*\"개판\"</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0.012*\"짜증\"</td>\n","      <td>0.010*\"밝기\"</td>\n","      <td>0.009*\"시리즈\"</td>\n","      <td>0.012*\"듯\"</td>\n","      <td>0.004*\"배경\"</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2405b70-a52a-4c8c-8989-11e26a4eaf29')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f2405b70-a52a-4c8c-8989-11e26a4eaf29 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f2405b70-a52a-4c8c-8989-11e26a4eaf29');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"t3KseH-PWqyP"}},{"cell_type":"code","source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"metadata":{"id":"4J81zHctKMMi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import gluonnlp as nlp\n","import pandas as pd\n","import numpy as np\n","import time\n","import random\n","import re\n","from tqdm.notebook import tqdm\n","from keras.utils import pad_sequences\n","\n","#kobert\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","\n","#transformers\n","from transformers import AdamW\n","from transformers import BertTokenizer\n","from transformers import get_linear_schedule_with_warmup\n","from transformers.optimization import get_cosine_schedule_with_warmup"],"metadata":{"id":"FE6djUXuYKYF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1CiJahNaZmVm","executionInfo":{"status":"ok","timestamp":1681821952968,"user_tz":-540,"elapsed":3,"user":{"displayName":"David Kim","userId":"08045787072475879019"}},"outputId":"49ae6335-3f6e-4ccf-dfbb-a0f768ce7617"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["_, vocab = get_pytorch_kobert_model()\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","\n","class KoBERTClassifier(nn.Module):\n","  def __init__(self, bert, hidden_size=768, num_classes=2, dr_rate=None, params=None):\n","    super(KoBERTClassifier, self).__init__()\n","\n","    self.bert = bert\n","    self.dr_rate = dr_rate\n","\n","    self.classifier = nn.Linear(hidden_size, num_classes)\n","    if dr_rate:\n","      self.dropout = nn.Dropout(p=dr_rate)\n","\n","  def gen_attention_mask(self, token_ids, valid_length):\n","    attention_mask = torch.zeros_like(token_ids)\n","    for i, v in enumerate(valid_length):\n","      attention_mask[i][:v] = 1\n","    return attention_mask.float()\n","\n","  def forward(self, token_ids, valid_length, segment_ids):\n","    attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","\n","    _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(), attention_mask=attention_mask.float().to(token_ids.device))\n","    if self.dr_rate:\n","      out = self.dropout(pooler)\n","    return self.classifier(out)\n","\n","    \n","def predict(predict_sentence, max_len=146, batch_size=1):\n","  data = [predict_sentence, '0']\n","  dataset_another = [data]\n","\n","  another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n","  test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size)\n","\n","  # load model\n","  model = torch.load(\"checkpoint_best.pt\", map_location=torch.device('cpu'))\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n","\n","  model.eval()\n","  with torch.no_grad():\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","      token_ids = token_ids.long().to(device)\n","      segment_ids = segment_ids.long().to(device)\n","\n","      valid_length = valid_length\n","      label = label.long().to(device)\n","\n","      out = model(token_ids, valid_length, segment_ids)\n","\n","      test_eval = []\n","      for i in out:\n","        logits = i\n","        logits = logits.detach().cpu().numpy()\n","\n","        if np.argmax(logits) == 0:\n","          test_eval.append(0)\n","        else:\n","          test_eval.append(1)\n","\n","  return(test_eval[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXWWJoKEKMK2","executionInfo":{"status":"ok","timestamp":1681824262202,"user_tz":-540,"elapsed":14936,"user":{"displayName":"David Kim","userId":"08045787072475879019"}},"outputId":"45362803-8037-490e-9790-571966e30985"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["using cached model. /content/drive/MyDrive/.cache/kobert_v1.zip\n","using cached model. /content/drive/MyDrive/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n","using cached model. /content/drive/MyDrive/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"]}]},{"cell_type":"code","source":["predict_list = []\n","for s in tqdm(df_ott.review_prep):\n","  pred = predict(s)\n","  print(f'{s}:', pred)\n","  \n","  predict_list.append(pred)"],"metadata":{"id":"V5GrK_AZh_GT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1jvaPRnWfk53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yrsA3qT2fk2U"},"execution_count":null,"outputs":[]}]}