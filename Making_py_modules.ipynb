{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22189,"status":"ok","timestamp":1684855515784,"user":{"displayName":"David Kim","userId":"08045787072475879019"},"user_tz":-540},"id":"-VkLJM9oOpuu","outputId":"4ede8575-17d8-45b7-d642-b3440c7fceaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Data 불러오기\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":405,"status":"ok","timestamp":1684855558585,"user":{"displayName":"David Kim","userId":"08045787072475879019"},"user_tz":-540},"id":"bJZ4atFNOptT","outputId":"5ae179e8-dc01-49db-aee1-c58156b7a2d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/OTT_team_project\n"]}],"source":["cd /content/drive/MyDrive/Colab Notebooks/OTT_team_project"]},{"cell_type":"markdown","metadata":{"id":"O5LxhTljQFe2"},"source":["## LDA 전처리 py파일 생성"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1684855558870,"user":{"displayName":"David Kim","userId":"08045787072475879019"},"user_tz":-540},"id":"x6BYYAd2Q8n5","outputId":"75d26f44-8099-42ed-a18a-e397834e80a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/OTT_team_project/utils\n"]}],"source":["cd ./utils/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":329,"status":"ok","timestamp":1684830132121,"user":{"displayName":"David Kim","userId":"08045787072475879019"},"user_tz":-540},"id":"dRBL0bPNOpqD","outputId":"f39b8f36-65f9-4be4-c2b7-2185327f3015"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing LDA_utils.py\n"]}],"source":["%%writefile LDA_utils.py\n","\n","## 전처리 필요 라이브러리\n","import re\n","import time\n","import datetime\n","import numpy as np\n","import pandas as pd\n","\n","# Tokenize\n","# from pykospacing import Spacing # 띄어쓰기\n","from gensim import corpora # 단어 빈도수 계산 패키지\n","import pyLDAvis.gensim_models # LDA 시각화용 패키지?\n","\n","# 한국어 형태소 분석기 중 성능이 가장 우수한 Mecab 사용\n","from konlpy.tag import *\n","mecab = Mecab()\n","\n","\n","## 토픽모델링 필요 라이브러리\n","import gensim\n","from gensim.corpora import Dictionary\n","from gensim.models import ldaseqmodel\n","from gensim.models import CoherenceModel\n","from gensim.models.callbacks import CoherenceMetric\n","from gensim.models.callbacks import PerplexityMetric\n","\n","# 기타\n","from tqdm import tqdm\n","import warnings # 경고 메시지 무시\n","import matplotlib.pyplot as plt\n","warnings.filterwarnings(action='ignore')\n","\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","\n","\n","## --------------- 데이터 전처리 --------------- ##\n","def date_extract(dataset, start_day:str, printing=False):\n","  \"\"\"\n","  Input: OTT별 전체 데이터프레임\n","  Output: 최근 n년의 기간에 해당하는 데이터 추출, 결측처리 완료\n","\n","  start_day : yyyy-mm-dd\n","  \"\"\"\n","\n","  dataset['at'] = pd.to_datetime(dataset['at'])\n","  dataset = dataset[dataset['at'] >= start_day]\n","  dataset = dataset.dropna(subset=['content'])\n","  dataset['year'] = dataset['at'].dt.year\n","  dataset = dataset.sort_values(by='at', ascending=True)\n","  \n","  if printing:\n","    for y in dataset['year'].unique():\n","      tmp = dataset[dataset['year'] == y]\n","      print(f'{y}: {len(tmp)} rows\\n')\n","  return dataset\n","\n","\n","class Data_processing:\n","  '''\n","  LDA 적용을 위한 모든 전처리\n","  '''\n","  def __init__(self, dataset):\n","    self.dataset = dataset\n","    self.replace_list = pd.read_excel('./replace_list.xlsx')\n","    self.stopwords_list = list(pd.read_excel('./stopword_list.xlsx')['stopword'])\n","\n","  def ko_language(self, text):\n","    # 한글 외 문자 제거 & 띄어쓰기 맞추기\n","    hangul = re.compile('[^가-힣 ]')\n","    result = hangul.sub('', text)\n","    return result\n","\n","  def replace_word(self, text):\n","    # 단어 치환\n","    for i, word in enumerate(self.replace_list['before_replacement']):\n","      if word in text:\n","        result = re.sub(word, self.replace_list['after_replacement'][i], text)\n","        return result\n","    return text\n","\n","  def tokenize(self, text):\n","    # 토큰화\n","    return mecab.nouns(text)\n","\n","  def remove_stopwords(self, text, add_stopwords=None):\n","    # 불용어제거\n","    if add_stopwords:\n","      self.stopwords_list += add_stopwords\n","    result = [x for x in text if x not in self.stopwords_list]\n","    return result\n","  \n","  def select_review(self, data, min_token_n:int, max_token_n:int):\n","    # 특정 토큰 개수의 리뷰 선별\n","    remove_idx_list = []\n","    for i in range(len(data)):\n","      if min_token_n <= len(data.iloc[i]['review_prep']) <= max_token_n:\n","        continue\n","      else:\n","        remove_idx_list.append(i)\n","\n","    return data.drop(remove_idx_list, axis=0)\n","\n","  def get_token(self, add_stopwords=None, min_token_n:int=3, max_token_n:int=1000):\n","    self.dataset['review_prep'] = self.dataset['content'].apply(lambda x:self.ko_language(x))\n","    self.dataset = self.dataset.reset_index(drop=True)\n","    print('ko_language done..')\n","    self.dataset['review_prep'] = self.dataset['review_prep'].apply(lambda x:self.replace_word(x))\n","    self.dataset = self.dataset.reset_index(drop=True)\n","    print('replace_word done..')\n","    self.dataset['review_prep'] = self.dataset['review_prep'].apply(lambda x:self.tokenize(x))\n","    self.dataset = self.dataset.reset_index(drop=True)\n","    print('tokenize done..')\n","    self.dataset['review_prep'] = self.dataset['review_prep'].apply(lambda x:self.remove_stopwords(x, add_stopwords))\n","    self.dataset = self.dataset.reset_index(drop=True)\n","    print('remove_stopwords done..')\n","    self.dataset = self.select_review(self.dataset, min_token_n, max_token_n)\n","    return list(self.dataset['review_prep']), self.dataset\n","\n","\n","\n","## ------------------ 모델링 ------------------ ##\n","class Model:\n","  '''\n","  LDA 모델\n","\n","  no_below = 분석에 사용할 단어의 최소 빈도 수 제약 (ex) 2이면, 빈도가 최소 2이상 넘어간 단어만 취급)\n","  no_above = 전체의 몇 %로 이상 차지하는 단어를 필터링 할 것인지?\n","  '''\n","  def __init__(self, inputs, num_topics:int, no_below:int=2):\n","    self.dictionary = corpora.Dictionary(inputs)\n","    self.dictionary.filter_extremes(no_below=no_below)\n","    self.corpus = [self.dictionary.doc2bow(x) for x in inputs]\n","    self.inputs = inputs\n","    self.num_topics = num_topics\n","\n","  def LDAseqmodel(self, timeslices, chunksize=2000, passes=10):\n","    '''\n","    CTM 함수\n","    '''\n","\n","    self.model = ldaseqmodel.LdaSeqModel(\n","        corpus=self.corpus,\n","        id2word=self.dictionary,\n","        time_slice=timeslices,\n","        num_topics=self.num_topics,\n","        chunksize=chunksize,\n","        passes=passes)\n","    \n","  \n","  def LDA_model(self, chunksize=2000, passes=20, iterations=400, eval_every=None):\n","    '''\n","    num_topics: 생성될 토픽의 개수\n","    chunksize: 한번의 트레이닝에 처리될 문서의 개수\n","    passes: 딥러닝에서 Epoch와 같은 개념으로, 전체 corpus로 모델 학습 횟수 결정\n","    interations: 문서 당 반복 횟수\n","    '''\n","    temp = self.dictionary[0]\n","    id2word = self.dictionary.id2token\n","\n","    self.model = LdaModel(\n","      corpus=self.corpus,\n","      id2word=id2word,\n","      chunksize=chunksize,\n","      alpha='auto',\n","      eta='auto',\n","      iterations=iterations,\n","      num_topics=self.num_topics,\n","      passes=passes,\n","      eval_every=eval_every)\n","\n","\n","  def print_topic_prop(self, topn=10, num_words=20):\n","    self.LDA_model()\n","    topics = self.model.print_topics(num_words=num_words)\n","\n","\n","    # 토픽별 포함 단어 추출\n","    topic_words = {}\n","    for idx, words in topics:\n","      topic_words[idx] = words.split('+')\n","\n","    topic_table = pd.DataFrame(topic_words)\n","    topic_table.columns = [f'topic_{t+1}' for t in range(len(topics))]\n","\n","    # coherence\n","    coherence_model_lda = CoherenceModel(model=self.model, texts=self.inputs, dictionary = self.dictionary, topn=10)\n","    coherence_lda = coherence_model_lda.get_coherence()\n","    print('LDA done..')\n","\n","    return topic_table, coherence_lda, topics"]},{"cell_type":"markdown","metadata":{"id":"d8MeaBO2qyih"},"source":["## 분류모델 추론"]},{"cell_type":"code","source":["%%writefile MODEL_utils.py\n","\n","## 전처리 필요 라이브러리\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","import pandas as pd\n","import re\n","import random\n","from tqdm import tqdm, tqdm_notebook\n","\n","# 자칫하면 오류날 수도 있는 부분\n","import gluonnlp as nlp\n","\n","# Model\n","from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","from transformers import ElectraForSequenceClassification, ElectraTokenizer\n","\n","\n","seed = 2021\n","deterministic = True\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","\n","# GPU 있으면 할당\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n","\n","######################### KoBERT #########################\n","bert = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n","tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n","\n","######################### KoELECTRA #########################\n","electramodel = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")  # KoELECTRA-Small-v3\n","tokenizer_electra = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n","\n","\n","# KoBERT 모델 입력 전처리\n","class BERTSentenceTransform:\n","    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n","        self._tokenizer = tokenizer\n","        self._max_seq_length = max_seq_length\n","        self._pad = pad\n","        self._pair = pair\n","        self._vocab = vocab \n","\n","    def __call__(self, line):\n","        # convert to unicode\n","        text_a = line[0]\n","        if self._pair:\n","            assert len(line) == 2\n","            text_b = line[1]\n","\n","        ##### 여기 수정!! #####\n","        tokens_a = self._tokenizer.tokenize(text_a)\n","        tokens_b = None\n","\n","        if self._pair:\n","            tokens_b = self._tokenizer(text_b)\n","\n","        if tokens_b:\n","            self._truncate_seq_pair(tokens_a, tokens_b,\n","                                    self._max_seq_length - 3)\n","        else:\n","            if len(tokens_a) > self._max_seq_length - 2:\n","                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n","        vocab = self._vocab\n","        tokens = []\n","        tokens.append(vocab.cls_token)\n","        tokens.extend(tokens_a)\n","        tokens.append(vocab.sep_token)\n","        segment_ids = [0] * len(tokens)\n","\n","        if tokens_b:\n","            tokens.extend(tokens_b)\n","            tokens.append(vocab.sep_token)\n","            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n","\n","        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The valid length of sentences. Only real  tokens are attended to.\n","        valid_length = len(input_ids)\n","\n","        if self._pad:\n","            # Zero-pad up to the sequence length.\n","            padding_length = self._max_seq_length - valid_length\n","            # use padding tokens for the rest\n","            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n","            segment_ids.extend([0] * padding_length)\n","\n","        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n","            np.array(segment_ids, dtype='int32')\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len, pad, pair):\n","        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","\n","\n","# KoBERT 모델 구조\n","class KoBERTClassifier(nn.Module):\n","  def __init__(self, bert, hidden_size=768, num_classes=2, dr_rate=None, params=None):\n","    super(KoBERTClassifier, self).__init__()\n","    self.bert = bert\n","    self.dr_rate = dr_rate\n","\n","    self.classifier = nn.Linear(hidden_size, num_classes)\n","    if dr_rate:\n","      self.dropout = nn.Dropout(p=dr_rate)\n","\n","  def forward(self, token_ids, valid_length, segment_ids, attention_mask):\n","    _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(), attention_mask=attention_mask.float().to(token_ids.device))\n","    if self.dr_rate:\n","      out = self.dropout(pooler)\n","    out = self.classifier(out)\n","    return out\n","\n","\n","# KoBERT 추론\n","def Custom_KoBERT_Predict(test:str, max_len=146, dr_rate=0.3):\n","  if '\"' in test:\n","    test = re.sub('\"', '', test)\n","\n","  test = [[test, '0']] # 입력 포맷\n","\n","  test_data = BERTDataset(test, 0, 1, tokenizer, vocab, max_len, True, False)\n","  test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1)\n","\n","  # model 불러오기\n","  model = KoBERTClassifier(bert, dr_rate=0.3).to(device)\n","\n","  model_state_dict = torch.load(\"./models/KoBERT_state_dict.pt\", map_location=device)\n","  model.load_state_dict(model_state_dict)\n","\n","  def gen_attention_mask(token_ids, valid_length):\n","    attention_mask = torch.zeros_like(token_ids)\n","    for i, v in enumerate(valid_length):\n","      attention_mask[i][:v] = 1\n","    return attention_mask.float()\n","\n","  # 테스트\n","  with torch.no_grad():\n","    model.eval()\n","\n","    token_ids, valid_length, segment_ids, label = next(iter(test_dataloader))\n","    \n","    token_ids = token_ids.long().to(device)\n","    segment_ids = segment_ids.long().to(device)\n","    label = label.long().to(device)\n","    valid_length = valid_length\n","\n","    attention_mask = gen_attention_mask(token_ids, valid_length)\n","\n","    out = model(token_ids, valid_length, segment_ids, attention_mask)\n","    softmax_out = nn.functional.softmax(out, dim=1)\n","    pred_label = softmax_out.argmax(dim=1).item()\n","\n","    return pred_label\n","\n","\n","######################### KoELECTRA #########################\n","# KoELECTRA 모델 입력 전처리\n","class ElectraClassificationDataset(Dataset):\n","  def __init__(self, test, tokenizer, max_len):\n","    self.input_data = list(test)\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","\n","  def __len__(self):\n","    return len(self.input_data)\n","\n","  def __getitem__(self, idx):\n","    inputs = self.tokenizer(self.input_data[idx], \n","                            return_tensors='pt', \n","                            truncation=True, \n","                            max_length=self.max_len, \n","                            padding='max_length',\n","                            add_special_tokens=True)\n","\n","    return {\n","        'input_ids': inputs['input_ids'][0],\n","        'attention_mask': inputs['attention_mask'][0],\n","        'token_type_ids': inputs['token_type_ids'][0]\n","          }\n","\n","\n","# KoELECTRA 추론\n","def Custom_KoELECTRA_Predict(test:str, max_len=146, dr_rate=0.3):\n","  if '\"' in test:\n","    test = re.sub('\"', '', test)\n","\n","  test_data = ElectraClassificationDataset(test, tokenizer_electra, max_len)\n","  test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1)\n","\n","  # model 불러오기\n","  model = electramodel.to(device)\n","\n","  model_state_dict = torch.load(\"./models/KoELECTRA_state_dict.pt\", map_location=device)\n","  model.load_state_dict(model_state_dict)\n","\n","  # 테스트\n","  with torch.no_grad():\n","    model.eval()\n","\n","    data = next(iter(test_dataloader))\n","    token_ids = data['input_ids'].long().to(device)\n","    token_type_ids = data['token_type_ids'].long().to(device)\n","    attention_mask = data['attention_mask'].long().to(device)\n","\n","    out = model(input_ids=token_ids, \n","                token_type_ids=token_type_ids, \n","                attention_mask=attention_mask)\n","    \n","    softmax_out = nn.functional.softmax(out.logits, dim=1)\n","    pred_label = softmax_out.argmax(dim=1).item()\n","    return pred_label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HRCgrTQGNgUp","executionInfo":{"status":"ok","timestamp":1684857404807,"user_tz":-540,"elapsed":7,"user":{"displayName":"David Kim","userId":"08045787072475879019"}},"outputId":"1045a4b9-54c0-4703-b7d1-3eae41ca2554"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting MODEL_utils.py\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z7mJ01V5qyKt"},"outputs":[],"source":["## 전처리 필요 라이브러리\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","import pandas as pd\n","import re\n","import random\n","from tqdm import tqdm, tqdm_notebook\n","\n","# 자칫하면 오류날 수도 있는 부분\n","import gluonnlp as nlp\n","\n","# Model\n","from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","from transformers import ElectraForSequenceClassification, ElectraTokenizer\n","\n","\n","seed = 2021\n","deterministic = True\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","\n","# GPU 있으면 할당\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n","\n","######################### KoBERT #########################\n","bert = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n","tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')\n","\n","######################### KoELECTRA #########################\n","electramodel = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")  # KoELECTRA-Small-v3\n","tokenizer_electra = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n","\n","\n","# KoBERT 모델 입력 전처리\n","class BERTSentenceTransform:\n","    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n","        self._tokenizer = tokenizer\n","        self._max_seq_length = max_seq_length\n","        self._pad = pad\n","        self._pair = pair\n","        self._vocab = vocab \n","\n","    def __call__(self, line):\n","        # convert to unicode\n","        text_a = line[0]\n","        if self._pair:\n","            assert len(line) == 2\n","            text_b = line[1]\n","\n","        ##### 여기 수정!! #####\n","        tokens_a = self._tokenizer.tokenize(text_a)\n","        tokens_b = None\n","\n","        if self._pair:\n","            tokens_b = self._tokenizer(text_b)\n","\n","        if tokens_b:\n","            self._truncate_seq_pair(tokens_a, tokens_b,\n","                                    self._max_seq_length - 3)\n","        else:\n","            if len(tokens_a) > self._max_seq_length - 2:\n","                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n","        vocab = self._vocab\n","        tokens = []\n","        tokens.append(vocab.cls_token)\n","        tokens.extend(tokens_a)\n","        tokens.append(vocab.sep_token)\n","        segment_ids = [0] * len(tokens)\n","\n","        if tokens_b:\n","            tokens.extend(tokens_b)\n","            tokens.append(vocab.sep_token)\n","            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n","\n","        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The valid length of sentences. Only real  tokens are attended to.\n","        valid_length = len(input_ids)\n","\n","        if self._pad:\n","            # Zero-pad up to the sequence length.\n","            padding_length = self._max_seq_length - valid_length\n","            # use padding tokens for the rest\n","            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n","            segment_ids.extend([0] * padding_length)\n","\n","        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n","            np.array(segment_ids, dtype='int32')\n","\n","class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len, pad, pair):\n","        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))\n","\n","\n","# KoBERT 모델 구조\n","class KoBERTClassifier(nn.Module):\n","  def __init__(self, bert, hidden_size=768, num_classes=2, dr_rate=None, params=None):\n","    super(KoBERTClassifier, self).__init__()\n","    self.bert = bert\n","    self.dr_rate = dr_rate\n","\n","    self.classifier = nn.Linear(hidden_size, num_classes)\n","    if dr_rate:\n","      self.dropout = nn.Dropout(p=dr_rate)\n","\n","  def forward(self, token_ids, valid_length, segment_ids, attention_mask):\n","    _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(), attention_mask=attention_mask.float().to(token_ids.device))\n","    if self.dr_rate:\n","      out = self.dropout(pooler)\n","    out = self.classifier(out)\n","    return out\n","\n","\n","# KoBERT 추론\n","def Custom_KoBERT_Predict(test:str, max_len=146, dr_rate=0.3):\n","  if '\"' in test:\n","    test = re.sub('\"', '', test)\n","\n","  test = [[test, '0']] # 입력 포맷\n","\n","  test_data = BERTDataset(test, 0, 1, tokenizer, vocab, max_len, True, False)\n","  test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1)\n","\n","  # model 불러오기\n","  model = KoBERTClassifier(bert, dr_rate=0.3).to(device)\n","\n","  model_state_dict = torch.load(\"./models/KoBERT_state_dict.pt\", map_location=device)\n","  model.load_state_dict(model_state_dict)\n","\n","  def gen_attention_mask(token_ids, valid_length):\n","    attention_mask = torch.zeros_like(token_ids)\n","    for i, v in enumerate(valid_length):\n","      attention_mask[i][:v] = 1\n","    return attention_mask.float()\n","\n","  # 테스트\n","  with torch.no_grad():\n","    model.eval()\n","\n","    token_ids, valid_length, segment_ids, label = next(iter(test_dataloader))\n","    \n","    token_ids = token_ids.long().to(device)\n","    segment_ids = segment_ids.long().to(device)\n","    label = label.long().to(device)\n","    valid_length = valid_length\n","\n","    attention_mask = gen_attention_mask(token_ids, valid_length)\n","\n","    out = model(token_ids, valid_length, segment_ids, attention_mask)\n","    softmax_out = nn.functional.softmax(out, dim=1)\n","    pred_label = softmax_out.argmax(dim=1).item()\n","\n","    return pred_label\n","\n","\n","######################### KoELECTRA #########################\n","# KoELECTRA 모델 입력 전처리\n","class ElectraClassificationDataset(Dataset):\n","  def __init__(self, test, tokenizer, max_len):\n","    self.input_data = list(test)\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","\n","  def __len__(self):\n","    return len(self.input_data)\n","\n","  def __getitem__(self, idx):\n","    inputs = self.tokenizer(self.input_data[idx], \n","                            return_tensors='pt', \n","                            truncation=True, \n","                            max_length=self.max_len, \n","                            padding='max_length',\n","                            add_special_tokens=True)\n","\n","    return {\n","        'input_ids': inputs['input_ids'][0],\n","        'attention_mask': inputs['attention_mask'][0],\n","        'token_type_ids': inputs['token_type_ids'][0]\n","          }\n","\n","\n","# KoELECTRA 추론\n","def Custom_KoELECTRA_Predict(test:str, max_len=146, dr_rate=0.3):\n","  if '\"' in test:\n","    test = re.sub('\"', '', test)\n","\n","  test_data = ElectraClassificationDataset(test, tokenizer_electra, max_len)\n","  test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=1)\n","\n","  # model 불러오기\n","  model = electramodel.to(device)\n","\n","  model_state_dict = torch.load(\"./models/KoELECTRA_state_dict.pt\", map_location=device)\n","  model.load_state_dict(model_state_dict)\n","\n","  # 테스트\n","  with torch.no_grad():\n","    model.eval()\n","\n","    data = next(iter(test_dataloader))\n","    token_ids = data['input_ids'].long().to(device)\n","    token_type_ids = data['token_type_ids'].long().to(device)\n","    attention_mask = data['attention_mask'].long().to(device)\n","\n","    out = model(input_ids=token_ids, \n","                token_type_ids=token_type_ids, \n","                attention_mask=attention_mask)\n","    \n","    softmax_out = nn.functional.softmax(out.logits, dim=1)\n","    pred_label = softmax_out.argmax(dim=1).item()\n","    return pred_label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wh8zfQ6EOpmv"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nH3jd5vIOplB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yu4QWZdFOfC9"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/zk5kfNscR/4E3YIU3mMW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}